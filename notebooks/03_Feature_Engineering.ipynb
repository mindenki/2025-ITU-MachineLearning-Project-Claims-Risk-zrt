{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to encode categorical features and we will also create our `Pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os \n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\"))) # adds folder to Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_data\n",
    "claims_raw, claims_raw_test = load_data(raw=True) # returns train and test dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's choose our y(target) variable. We want to try multiple things out so we will create it flexibly.\n",
    "\n",
    "1. Simply `ClaimNb`.\n",
    "<br/><br/>\n",
    "2. ClaimRate $\\ = \\frac{ClaimNb}{Exposure} $ . \n",
    "<br/><br/>\n",
    "3. log_ClaimRate $\\ =  \\log(1+ \\frac{ClaimNb}{min(Exposure, threshold)})   $. This accounts for the big skew and also caps some extreme values. We use 0.01 as a floor because it is widely used in insurance pricing as it reduces extreme variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, in all cases, `Exposure` is capped at 1.0, as it shouldn't exceed it. We also clip it at 0.01 as minimum, because the values lead to extreme targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make some basic transformations on the features before creating the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.preprocess import preprocess_manual, build_feature_pipeline, FEATURES\n",
    "\n",
    "\n",
    "claims_processed = preprocess_manual(claims_raw, exposure=False) \n",
    "#claims_processed.info()\n",
    "# save the processed data\n",
    "#claims_processed.to_csv(f\"../data/processed/claims_processed_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing.Literal['ClaimNb', 'ClaimRate', 'log_ClaimRate']\n"
     ]
    }
   ],
   "source": [
    "from src.preprocess.preprocess import create_target, TargetType\n",
    "print(TargetType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose our target variable that we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['src.preprocess.preprocess'])\n",
    "\n",
    "target = \"log_ClaimRate\" # <-- choose target here from above\n",
    "\n",
    "y_train = create_target(claims_raw, target=target)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here the features and which transformations we want on them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Exposure`: Exposure is only used as a feature when it's part of the Target(e.g when target is `ClaimNb`). We choose to floor it at .01, as there are some extreme values. We do not use any more transformations.\n",
    "\n",
    "-  `VehPower`: Vehicle power has 11 categories ranging from 4 to 15. We choose to make 7 bins out of this(4,5,6,7,8-9,10-11,12+), as higher power cars are rarer, so this makes it more monotic. Then we apply a ordinal encoding function, as we have a clear lower/higher structure.\n",
    "\n",
    "- `VehAge`: Vehicle age is very right skewed, with older cars ranging to 100 y.o. We choose to try binning it into 8 bins(0-1,2-3,4-5,6-10, 11-15, 16-20, 21-30 and 30+), as data is better distributed. After that ordinal encoding should be applied. We also try using it as a numerical feature.\n",
    "\n",
    "-  `DrivAge`: Driver age looks pretty normal compared to every other feature, with a mild skew only(.4). We decided to keep it without transformation. \n",
    "\n",
    "- `Density`: We use logarithm transformation on the denisty feature, as the log transformed feature loses its skewed distribution(.05).\n",
    "\n",
    "- `Area`: Area is fully determined by Density ranges. Logarithm scatterplots show clear stripes, so we decided to drop Area, as it's redundant.\n",
    "\n",
    "- `BonusMalus`: More than half of the data has a Bonus Malus score of 50. It is hard to make any transformations to make the feature better. We decided to keep it as it is.\n",
    "\n",
    "- `VehBrand`: As we don't have any ordinal structure in this feature, we can not bin or ordinal encode the feature, so we decide to keep it as it is and one-hot encode it.\n",
    "\n",
    "- `VehGas:`: The ratio of regular and diesel is almost 50/50, thats the only 2 value types, so we one-hot encode it to make a it a binary feature.\n",
    "\n",
    "- `Region`: Region has a lot of categories, and a many of them contain very low counts(<1%). We decided to group these into a seperate category so we can reduce dimensionality. Then we use one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_configs_filtered = [\n",
    "    f for f in FEATURES if f.name != \"Exposure\" or target == \"ClaimNb\"\n",
    "]\n",
    "\n",
    "preprocesser = build_feature_pipeline(feature_configs_filtered)\n",
    "\n",
    "X_train = preprocesser.fit_transform(claims_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/preprocesser_log_ClaimRate.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(f\"../data/processed/X_train.npz\", X_train)\n",
    "np.save(f\"../data/processed/y_train_{target}.npy\", y_train.values)\n",
    "\n",
    "\n",
    "joblib.dump(preprocesser, f\"../models/preprocesser_{target}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with the test datasets, to get their processed version as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = create_target(claims_raw_test, target=target)\n",
    "X_test = preprocesser.transform(preprocess_manual(claims_raw_test, exposure=False))\n",
    "\n",
    "sparse.save_npz(f\"../data/processed/X_test.npz\", X_test)\n",
    "np.save(f\"../data/processed/y_test_{target}.npy\", y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m a, b = load_data(raw=\u001b[38;5;28;01mFalse\u001b[39;00m, target=target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/3rd semester/Machine Learning/2025-ITU-MachineLearning-Project-Claims-Risk-zrt/src/data/load_data.py:59\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(raw, target)\u001b[39m\n\u001b[32m     57\u001b[39m X_train = sparse.load_npz(X_train_path)\n\u001b[32m     58\u001b[39m X_test = sparse.load_npz(X_test_path)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m y_train = pd.read_numpy(y_train_path)\n\u001b[32m     60\u001b[39m y_test = pd.read_numpy(y_test_path)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (X_train, y_train), (X_test, y_test)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pandas' has no attribute 'read_numpy'"
     ]
    }
   ],
   "source": [
    "a, b = load_data(raw=False, target=target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claims_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
