{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to encode categorical features and we will also create our `Pipeline` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os \n",
    "import importlib\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\"))) # adds folder to Python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_data import load_data\n",
    "claims_raw, claims_raw_test = load_data(raw=True) # returns train and test dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's choose our y(target) variable. We want to try multiple things out so we will create it flexibly.\n",
    "\n",
    "1. Simply `ClaimNb`.\n",
    "<br/><br/>\n",
    "2. ClaimRate $\\ = \\frac{ClaimNb}{Exposure} $ . \n",
    "<br/><br/>\n",
    "3. log_ClaimRate $\\ =  \\log(1+ \\frac{ClaimNb}{min(Exposure, threshold)})   $. This accounts for the big skew and also caps some extreme values. We use 0.01 as a floor because it is widely used in insurance pricing as it reduces extreme variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, in all cases, `Exposure` is capped at 1.0, as it shouldn't exceed it. We also clip it at 0.01 as minimum, because the values lead to extreme targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make some basic transformations on the features before creating the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess.preprocess import preprocess_manual, build_feature_pipeline, FEATURES\n",
    "\n",
    "\n",
    "claims_processed = preprocess_manual(claims_raw, exposure=False) \n",
    "#claims_processed.info()\n",
    "# save the processed data\n",
    "#claims_processed.to_csv(f\"../data/processed/claims_processed_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing.Literal['ClaimNb', 'ClaimRate', 'log_ClaimRate']\n"
     ]
    }
   ],
   "source": [
    "from src.preprocess.preprocess import create_target, TargetType\n",
    "print(TargetType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose our target variable that we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(sys.modules['src.preprocess.preprocess'])\n",
    "\n",
    "target = \"log_ClaimRate\" # <-- choose target here from above\n",
    "\n",
    "y_train = create_target(claims_raw, target=target)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here the features and which transformations we want on them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Exposure`: Exposure is only used as a feature when it's part of the Target(e.g when target is `ClaimNb`). We choose to floor it at .01, as there are some extreme values. We do not use any more transformations.\n",
    "\n",
    "-  `VehPower`: Vehicle power has 11 categories ranging from 4 to 15. We choose to make 7 bins out of this(4,5,6,7,8-9,10-11,12+), as higher power cars are rarer, so this makes it more monotic. Then we apply a ordinal encoding function, as we have a clear lower/higher structure.\n",
    "\n",
    "- `VehAge`: Vehicle age is very right skewed, with older cars ranging to 100 y.o. We choose to try binning it into 8 bins(0-1,2-3,4-5,6-10, 11-15, 16-20, 21-30 and 30+), as data is better distributed. After that ordinal encoding should be applied. We also try using it as a numerical feature.\n",
    "\n",
    "-  `DrivAge`: Driver age looks pretty normal compared to every other feature, with a mild skew only(.4). We decided to keep it without transformation. \n",
    "\n",
    "- `Density`: We use logarithm transformation on the denisty feature, as the log transformed feature loses its skewed distribution(.05).\n",
    "\n",
    "- `Area`: Area is fully determined by Density ranges. Logarithm scatterplots show clear stripes, so we decided to drop Area, as it's redundant.\n",
    "\n",
    "- `BonusMalus`: More than half of the data has a Bonus Malus score of 50. It is hard to make any transformations to make the feature better. We decided to keep it as it is.\n",
    "\n",
    "- `VehBrand`: As we don't have any ordinal structure in this feature, we can not bin or ordinal encode the feature, so we decide to keep it as it is and one-hot encode it.\n",
    "\n",
    "- `VehGas:`: The ratio of regular and diesel is almost 50/50, thats the only 2 value types, so we one-hot encode it to make a it a binary feature.\n",
    "\n",
    "- `Region`: Region has a lot of categories, and a many of them contain very low counts(<1%). We decided to group these into a seperate category so we can reduce dimensionality. Then we use one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (542410, 1), indices imply (542410, 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m X_train = preprocessor.fit_transform(claims_processed)\n\u001b[32m      8\u001b[39m clean_names = [name.split(\u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m preprocessor.get_feature_names_out()]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_train_df = pd.DataFrame(X_train, columns=clean_names)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[32m     12\u001b[39m X_train_df.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../data/processed/claims_features_train.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/claims_ml/lib/python3.11/site-packages/pandas/core/frame.py:867\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    859\u001b[39m         mgr = arrays_to_mgr(\n\u001b[32m    860\u001b[39m             arrays,\n\u001b[32m    861\u001b[39m             columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m             typ=manager,\n\u001b[32m    865\u001b[39m         )\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m         mgr = ndarray_to_mgr(\n\u001b[32m    868\u001b[39m             data,\n\u001b[32m    869\u001b[39m             index,\n\u001b[32m    870\u001b[39m             columns,\n\u001b[32m    871\u001b[39m             dtype=dtype,\n\u001b[32m    872\u001b[39m             copy=copy,\n\u001b[32m    873\u001b[39m             typ=manager,\n\u001b[32m    874\u001b[39m         )\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    876\u001b[39m     mgr = dict_to_mgr(\n\u001b[32m    877\u001b[39m         {},\n\u001b[32m    878\u001b[39m         index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    881\u001b[39m         typ=manager,\n\u001b[32m    882\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/claims_ml/lib/python3.11/site-packages/pandas/core/internals/construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m _check_values_indices_shape_match(values, index, columns)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/claims_ml/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (542410, 1), indices imply (542410, 35)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_configs_filtered = [\n",
    "    f for f in FEATURES if f.name != \"Exposure\" or target == \"ClaimNb\"\n",
    "]\n",
    "\n",
    "preprocessor = build_feature_pipeline(feature_configs_filtered)\n",
    "\n",
    "X_train = preprocessor.fit_transform(claims_processed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VehPower__VehPower' 'VehAge__VehAge' 'DrivAge__DrivAge'\n",
      " 'Density__Density' 'BonusMalus__BonusMalus' 'VehBrand__VehBrand_B1'\n",
      " 'VehBrand__VehBrand_B10' 'VehBrand__VehBrand_B11'\n",
      " 'VehBrand__VehBrand_B12' 'VehBrand__VehBrand_B13'\n",
      " 'VehBrand__VehBrand_B14' 'VehBrand__VehBrand_B2' 'VehBrand__VehBrand_B3'\n",
      " 'VehBrand__VehBrand_B4' 'VehBrand__VehBrand_B5' 'VehBrand__VehBrand_B6'\n",
      " 'VehGas__VehGas_Diesel' 'VehGas__VehGas_Regular' 'Region__Region_Other'\n",
      " 'Region__Region_R11' 'Region__Region_R22' 'Region__Region_R23'\n",
      " 'Region__Region_R24' 'Region__Region_R25' 'Region__Region_R26'\n",
      " 'Region__Region_R31' 'Region__Region_R41' 'Region__Region_R52'\n",
      " 'Region__Region_R53' 'Region__Region_R54' 'Region__Region_R72'\n",
      " 'Region__Region_R73' 'Region__Region_R82' 'Region__Region_R91'\n",
      " 'Region__Region_R93']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(f\"../data/processed/X_train.npz\", X_train)\n",
    "np.save(f\"../data/processed/y_train_{target}.npy\", y_train.values)\n",
    "\n",
    "\n",
    "joblib.dump(preprocessor, f\"../models/preprocesser_{target}.joblib\")x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with the test datasets, to get their processed version as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = create_target(claims_raw_test, target=target)\n",
    "X_test = preprocessor.transform(preprocess_manual(claims_raw_test, exposure=False))\n",
    "X_test_df = pd.DataFrame(X_test, columns=clean_names)\n",
    "\n",
    "sparse.save_npz(f\"../data/processed/X_test.npz\", X_test)\n",
    "np.save(f\"../data/processed/y_test_{target}.npy\", y_test.values)\n",
    "\n",
    "X_test_df.to_csv(f\"../data/processed/claims_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: mean=1.4890470308438266, std=1.5364414490919565\n",
      "Feature 1: mean=2.34988661713464, std=1.621879527772838\n",
      "Feature 2: mean=-2.000133065569051e-16, std=1.0\n",
      "Feature 3: mean=-9.18455182629008e-16, std=0.9999999999999999\n",
      "Feature 4: mean=1.0912079402816383e-16, std=1.0\n",
      "Feature 5: mean=0.24014306520897477, std=0.427170192594255\n",
      "Feature 6: mean=0.026216330819859517, std=0.15977807990523357\n",
      "Feature 7: mean=0.02007521985214137, std=0.1402576393642412\n",
      "Feature 8: mean=0.24489039656348519, std=0.4300221973741171\n",
      "Feature 9: mean=0.017846278645305212, std=0.13239255637617747\n",
      "Feature 10: mean=0.005975184823288657, std=0.07706803481091366\n",
      "Feature 11: mean=0.23565384119024355, std=0.42440677223923634\n",
      "Feature 12: mean=0.07868955218377242, std=0.2692536101167258\n",
      "Feature 13: mean=0.03721539057170775, std=0.18928921067061114\n",
      "Feature 14: mean=0.05134492358179237, std=0.22070029996393822\n",
      "Feature 15: mean=0.04194981655942921, std=0.20047451072408048\n",
      "Feature 16: mean=0.4898268837226452, std=0.4998964969923348\n",
      "Feature 17: mean=0.5101731162773547, std=0.49989649699233474\n",
      "Feature 18: mean=0.03082723401117236, std=0.17284940166049978\n",
      "Feature 19: mean=0.10321343633045113, std=0.30423744491977156\n",
      "Feature 20: mean=0.011793661621282793, std=0.1079563391693395\n",
      "Feature 21: mean=0.012971737246732177, std=0.11315242498300207\n",
      "Feature 22: mean=0.23690934901642668, std=0.42518620552063957\n",
      "Feature 23: mean=0.016054276285466712, std=0.12568427307510122\n",
      "Feature 24: mean=0.015405320698364706, std=0.12315842152506355\n",
      "Feature 25: mean=0.04021496653822754, std=0.19646303215759645\n",
      "Feature 26: mean=0.01912944082889327, std=0.13697994496446242\n",
      "Feature 27: mean=0.05704540845485887, std=0.2319293638784818\n",
      "Feature 28: mean=0.06202503641157058, std=0.24120101838448763\n",
      "Feature 29: mean=0.028071016389815823, std=0.16517576828535868\n",
      "Feature 30: mean=0.0460260688409137, std=0.20954157064402562\n",
      "Feature 31: mean=0.02531295514463229, std=0.15707389804317623\n",
      "Feature 32: mean=0.12494607400306042, std=0.3306577574990221\n",
      "Feature 33: mean=0.05295256355893144, std=0.22393880764947544\n",
      "Feature 34: mean=0.1171014546191995, std=0.32154113880694496\n"
     ]
    }
   ],
   "source": [
    "a = X_train.todense()\n",
    "for i in range(a.shape[1]):\n",
    "    col = a[:, i]\n",
    "    print(f\"Feature {i}: mean={np.mean(col)}, std={np.std(col)}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claims_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
